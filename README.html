<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>multinomial naive bayes classifier</title>
    
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />

    <style>
        body {
            max-width: 40em;
            padding: 2em 10%;
            margin: 0 auto;
            font: 1.2em/1.62 sans-serif;
            color: #444;
            background-color: rgb(241, 240, 236); 
        }

        h1, h2, h3 {
            line-height: 1.2;
            margin-bottom: 1.5rem;
            color: #222;
        }

        p {
            margin-top: 20px;
            margin-bottom: 50px;
            text-align: justify;
            hyphens: auto;
        }

        aside {
            color: rgb(121, 121, 121);
            font-size: 0.8em;
            margin-bottom: 1em;
        }

        a {
            text-decoration: underline;
            color: rgb(51, 59, 97);
        }

        pre[class*="language-"] {
            background: rgba(255, 255, 255, 0.7) !important; 
            border: 1px solid rgba(0, 0, 0, 0.05) !important;
            border-left: 5px solid rgb(51, 59, 97) !important; 
            
            padding: 1.5em !important;
            margin: 2em 0 !important;
            border-radius: 4px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            overflow: auto;
        }

        code[class*="language-"] {
            font-family: 'Consolas', 'Monaco', 'Andale Mono', monospace !important;
            font-size: 0.9em !important;
            text-shadow: none !important; 
        }

        .token.keyword { color: #859900; font-weight: bold; }
        .token.string { color: #2aa198; }
        .token.comment { color: #93a1a1; }
        .token.function { color: #268bd2; }

        .output-block {
            background-color: rgba(39, 7, 7, 0.026);
            border: 1px dashed #ccc; 
            color: #666;
            padding: 1.2em;
            margin: 1em 0 2em 0;
            font-size: 0.9em;
            word-break: break-all;
            line-height: 1.4;
            border-radius: 4px;
        }
        .output-label {
            font-size: 0.9em;
            font-weight: bold;
            color: #999;
            margin-bottom: 5px;
            display: block;
        }
    </style>
</head>
<body>

    <header>
        <h1>Textmood Classifier</h1>
    </header>
    
    <aside>Dec 2025</aside>
    <p class="description">    
        I recently started building a tool to detect emotion in text
        (<a href="https://github.com/soradacroi/ai-emotion-detection-py">github/ai-emotion-detection-py</a>).
        As I was looking into implementation I stumbled upon 
        <b><a target="_blank" style="color: #3891f0;" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">
            naive bayes classifier</a></b>.
        While making the emotion detection I thought to myself why not make a library out of it,
        I have never made a proper library, but i tried this time... idk what i want to write...
        <br>
        So after <i>finishing</i> I made this library. <span style="color: #e67331;">(LIBRARY???????????????)</span>
        <br><br>
        More resources:
        <ul>
            <li><a target="_blank" href="https://youtu.be/O2L2Uv9pdDA?si=24xVgaPAKTjpbXCt">video by StatQuest with Josh Starmer</video></a></li>
            <li><a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html">Scikit-learn Documentation</a></li>
        </ul>
    </p>

    <p>
        Source code: <br>
        <a href="https://github.com/soradacroi/textmood-classifier">github/textmood-classifier</a>
        <br><br>
    </p>



    <h2>Negation and Stopwords</h2>
    <pre><code class="language-python">
    for t in tokens:
        if t in self.stop_words:
            continue
        if t in self.negations:
            negate = True
            continue
        if negate:
            processed_tokens.append("NOT_" + t)
            negate = False 
        else:
            processed_tokens.append(t)
    </code></pre>

    <p>
        so this "libary" does like how do i explain this...
        <br>
        So this library tracks negation words and transform the next word as like an unique word,
        for example a sentence "hello i am not happy" will get transform like
        "hello i am NOT_happy", that way it have some context as multinomial naive bayes is not
        context aware. <span style="color: rgb(242, 238, 232);">(the n-gram (later) does the same thing but because i 
        am the best human (no i am not) what i am doing is always better wahahahahaha)</span><br>
        And well the stop words help with the negation like,<br>
        Let "i am not very happy" be a data, let "very" be a stop word so, 
        after transformation it will be "i am NOT_happy" very got removed and made "not very happy" mean
        sad so it can now have more complex data.<span style="color: rgb(242, 238, 232);"> ig i hope</span>
    </p>

    <h2>N-gram Generation</h2>
    <pre><code class="language-python">      
    all_features = []
    min_n, max_n = self.ngram_range
    
    n_tokens = len(processed_tokens)
    
    for n in range(min_n, max_n + 1):
        if n == 1:
            all_features.extend(processed_tokens)
        else:
            for i in range(n_tokens - n + 1):
                gram = "_".join(processed_tokens[i : i + n])
                all_features.append(gram)</code></pre>
    <p>
        The n-gram generation works like making two (more depends on your given depth,
        or just one if u do (1, 1)) one word or token,for example:<br>
        "i am not very happy" (using the example from <i>#Negation and Stopwords</i>)
        <br>the Negation and Stopwords will generate [i, am, NOT_happy] then,<br>
        N-gram generator will output something like (if depth is (1, 2)):<br>
        [i, am, NOT_happy, i_am, am_NOT_happy] increasing the tokens and context.
        While increasing the depth will increase the context, it will also increase 
        the size of the self.vocab set and the model.pkl file. Also the problem of
        <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> comes up,
        they require significantly larger datasets to avoid overfitting.
        An ngram_range of (1, 2) or (1, 3) provides the best balance between
        understanding context and remaining generalized. <i style="color: #cf9faa;">(for me atleast)</i>
    </p>

    <h2>Basic set-up</h2>
    <pre><code class="language-python">
from textmood import TextMood, load_data

# 1. Configuration
stop_words={"the", "is", "very"}, 
negations={"not", "no", "never"},
ngram_range=(1, 2)


# Training data: (text, label)
data = [("I love this", "pos"), ("not good", "neg")]
model = TextMood(stop_words=stop_words, negations=negations, ngram_range=ngram_range)
model.train(data, labels=["pos", "neg"])

# Predict
print(model.predict("The movie not good"))
    </code></pre>
    <p>
        You can also load data from your dataset (a csv, text or anything just need to have separators
        and same labels as your targeted labels) by using the <span style="color: #e67331;">load_data</span> function.
    </p>
    <pre><code class="language-python">
data = load_data(dataset_path, amount, labels, separator)
    </code></pre>
    <h2>
        Save and Load
    </h2>
    <pre><code class="language-python">
model.save_model("my_model.pkl")

# later... 
model = TextMood()
model.load_model("my_model.pkl")
print(model.predict(input()))
    </code></pre>
    <h2>Laplace Smoothing</h2>
    <pre><code class="language-python">
# From your predict function:
p_feat_given_class = (feat_count_in_class + smooth) / (total_feat_in_class + (vocab_size * smooth))
    </code></pre>
    <p>
        If you try to predict a sentence with a word the model has never seen before,
        the probability for that word becomes 0. Since we multiply probabilities (and it never saw
        that word so probability of that word is ZERO), a single zero would destroy the entire calculation.
        <br>
        <b>Laplace Smoothing</b> (controlled by the <code>smooth</code> parameter) adds a tiny 
        number to every count. This ensures every word has at least a <i>tiny</i> chance of 
        existing in every category, preventing the model from failing on new data.
    </p>

    <br>
    <h2>
        Limitations and all
    </h2>

    <li>
    If you train your model with 1000 positive reviews but only 100 negative reviews,
    the priors will get skewed, so try to have same numbers of label counts,
    <span style="color: #e67331;">use the amount from the load_data to get equal data for 
        all the labels.
    </span></li>
    <li>A sentence like "Oh great, another delay, just what I wanted!" contains the words "great" 
    and "wanted." The model will likely see these as highly positive features and guess "positive"
    missing the sarcasm</li>
    <li>
        Currently, the model ignores symbols (like ! or ?) and also numbers, so it might lose 
        the <i>"intensity"</i> of the emotion.
    </li>

    <p>Thank you.</p>


    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

</body>
</html>